# -*- coding: utf-8 -*-
"""Week2.SOC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K9Cez7T7VxuofmXjf1CQCcg9SM3TkbNi
"""

test = "Hello World"
print("test: " + test)

"""sigmoid function"""

#eg
import math
def basic_sigmoid(x):
  s=1/(1+math.exp(-x))
  return s
basic_sigmoid(3)

x=[1,2,3]
basic_sigmoid(x) #this will give error because x is a vector

import numpy as np
x=np.array([1,2,3])
print(np.exp(x)) #this will give exp(1),exp(2),exp(3)

x=np.array([1,2,3])
print(x+3)

def sigmoid(x):
  s = 1/(1+np.exp(-x))
  return s
x=np.array([1,2,3])
sigmoid(x)

def sigmoid_derivative(x):
  s=sigmoid(x)
  ds=s*(1-s)
  return ds
x=np.array([1,2,3])
print("sigmoid_derivative = " + str(sigmoid_derivative(x)))

def image2vector(image):
  v=image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)
  return v

#we often normalize our data because it leads to better performance as gradient descent converges faster after normalizing
def normlizeR(x):
  x_norm = np.linalg.norm(x,axis=1,keepdims=True)
  x=x/x_norm
  return x
x=np.array([[0,3,4],[1,6,4]])
print("normlized rows: " + str(normlizeR(x)))

#softmax is used when our alogo needs to classify two or more classes
def softmax(x):
  x_exp = np.exp(x)
  x_sum = np.sum(x_exp,axis=1,keepdims=True)
  s = x_exp/x_sum
  return s
x= np.array([[9,2,5,0,0],[7,5,0,0,0]])
print("softmax(x) =", softmax(x))

import time

x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###
tic = time.process_time()
dot = 0
for i in range(len(x1)):
    dot+= x1[i]*x2[i]
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC OUTER PRODUCT IMPLEMENTATION ###
tic = time.process_time()
outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros
for i in range(len(x1)):
    for j in range(len(x2)):
        outer[i,j] = x1[i]*x2[j]
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC ELEMENTWISE IMPLEMENTATION ###
tic = time.process_time()
mul = np.zeros(len(x1))
for i in range(len(x1)):
    mul[i] = x1[i]*x2[i]
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###
W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array
tic = time.process_time()
gdot = np.zeros(W.shape[0])
for i in range(W.shape[0]):
    for j in range(len(x1)):
        gdot[i] += W[i,j]*x1[j]
toc = time.process_time()
print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

import time
x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### VECTORIZED DOT PRODUCT OF VECTORS ###
tic = time.process_time()
dot = np.dot(x1,x2)
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED OUTER PRODUCT ###
tic = time.process_time()
outer = np.outer(x1,x2)
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED ELEMENTWISE MULTIPLICATION ###
tic = time.process_time()
mul = np.multiply(x1,x2)
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED GENERAL DOT PRODUCT ###
tic = time.process_time()
dot = np.dot(W,x1)
toc = time.process_time()
print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

#L1
def L1(yh,y):
  loss = sum(abs(yh-y))
  return loss
def L2(yh,y):
  x=yh-y
  loss=np.dot(x,x)
  return loss

